# Lecture 4 Summary - kNNs and SVM RBFs
- **explain the notion of similarity-based algorithms**
	- make predictions based on the similarity of input data to training data
		- e.g. [[k-Nearest Neighbours]] 
	- rely on the assumption that similar data points are likely to have similar output values
	- similarity often measured by distance metrics such as Euclidean distance
- **broadly describe how $k$-NNs use distances**
	- uses distance metrics to find $k$ closest training points to an input point
	- prediction for the test point made based on output values of the nearest neighbours
- **discuss the effect of using a small/large value of the hyperparameter $k$ when using the $k$-NN algorithm**
	- smaller $k$ can lead to overfitting
		- predictions are based on a small number of neighbours
	- larger $k$ can smooth out predictions and lead to more generalized results
		- can blur boundaries and even underfit
- **describe the problem of curse of dimensionality**
	- more features results in exponential increase in volume of space
		- data becomes sparse
		- kNN has difficulty finding close neighbours, as all points become almost equidistant
	- more features mean relevant features can be drowned out by accidental similarities in irrelevant features
	- see [[k-Nearest Neighbours#Curse of Dimensionality]]
- **explain the general idea of SVMs with RBF kernel**
	- [[Support Vector Machines (SVM) with RBF Kernel]]
	- used for non-linear classification
	- RBF kernel transforms input space into higher-dimensional space
		- non-linear data in a higher-dimensional space may become linearly separable
		- easier to separate data using hyperplane
	- SVM aims to find optimal separating hyperplane that maximises margin between different classes
- **broadly describe the relation of `gamma` and `C` hyperparameters of SVMs with the fundamental tradeoff**
	- `gamma`
		- controls how far the influence of a single training example reaches
		- higher gamma leads to more complexity
			- decision boundary may bend around individual data points
			- may overfit
	- `C`
		- regularization parameter or penalty parameter
		- trade-off between maximizing margin between hyperplane and closest data points, and minimizing errors
			- larger margin improves performance and generalization
		- smaller `C` allows for more misclassificaton
			- higher bias but lower variance
			- simpler decision boundary
			- prioritize minimizing errors over maximizing margin
			- can cause underfitting
		- larger `C` aims for lower misclassification rate
			- lower bias but higher variance
			- prioritize maximizing margin over minimizing errors
			- can cause overfitting
		- [[Bias and Variance Tradeoff]]
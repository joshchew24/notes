# Lecture 3 Summary - Machine Learning Fundamentals
## Learning Objectives
- **explain how decision boundaries change with the `max_depth` hyperparameter**
	- decision boundaries in decision trees are determined by conditions set at each node
	- `max_depth` controls maximum depth of the tree
	- deeper trees have more complex decision boundaries, capturing finer details in data
	- too much depth can cause overfitting
- **explain the concept of generalization**
	- a model's ability to perform well on new, unseen data, not just training data
	- captures underlying patterns in data without being too complex/simplistic
- **appropriately [[Data Splitting|split a dataset]] into tran and test sets using `train_test_split` function**
	- `train_test_split` function in [[sklearn]] is used to divide a dataset into training and testing sets
	- training set used to train the model
	- testing set used to evaluate its performance
	- ensures we are evaluating performance on unseen data
- **explain the difference between train, validation, test, and "deployment" data**
	- **Training Data**: used to fit the model
	- **Validation Data**: used to tune hyperparameters and make decisions about the model (e.g. which features to use)
	- **Test Data**: used to assess model's performance and its ability to generalize to new data
	- **Deployment Data**: real world data encountered by the model after deployment in production environment
- **identify the difference between training error, validation error, and test [[Error]]**
	- **Training Error**: error the model makes on the data it was trained on
	- **Validation Error**: error during hyperparameter tuning phase on a separate validation set
	- **Test Error**: error when the model is applied to the test set, which reflects its ability to generalize
- **explain [[Cross Validation]] and use `cross_val_score` and `cross_validate` to calculate cross-validation error**
	- create many different data splits, training a model for each split and validating it on the associated remainder
	- `cross_val_score` and `cross_validate` automate this and provide a more robust esetimate of the model's performance
- **recognize [[Overfitting]] and/or [[Underfitting]] by looking at train and test scores**
	- when a model performs well on training data but poorly on test data, it is likely overfitting (too complex)
	- when a model performs poorly on both datasets, it is likely underfitting (too simple)
- **explain why it is generally not possible to get a perfect test score (zero test error) on a supervised learning problem**
	- perfect test scores are generally unattainable in supervised learning
		- the presence of noise in real-world data
		- limitations of the model's ability to capture all underlying patterns in complex data
- **describe the [[Bias and Variance Tradeoff|Fundamental Tradeoff]] between training score and the train-test gap**
	- we want to achieve a high training score, and minimize the gap between training and test performance
	- model with high training score but a large gap often indicates overfitting
- **state the #GoldenRule**
	- never train on testing data
- **start to build a standard recipe for supervised learning: train/test split, hyperparameter tuning with cross-validation, test on test set**
	- Train/Test split
		- separate your data into training and testing sets
	- Hyperparameter Tuning with Cross-Validation
		- use cross-validation to determine the best hyperparameters for your model
	- Test on Test Set
		- evaluate model's performance on test set to understand how well it generalizes to new data
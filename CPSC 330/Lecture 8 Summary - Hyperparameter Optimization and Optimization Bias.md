- **explain the need for [[Hyperparameters]] optimization**
	- helps enhance performance of the model
		- efficiency in training
		- prediction quality
		- model complexity
	- find the most effective and efficient model for a givein dataset
- **carry out hyperparameter optimization using `sklearn`'s `GridSearchCV` and `RandomizedSearchCV`**
	- [[Hyperparameters#Exhaustive Grid Search|Grid Search]]
		- systematically tests all possible combinations of hyperparameters (from given set)
		- determines which combination gave best performance based on specified scoring method
	- [[Hyperparameters#Randomized Hyperparameter Search|Randomized Search]]
		- randomly samples hyperparameter values to test from given distributions
		- tests a specified number of combinations
			- or until a time limit is reached
		- faster than GridSearch
			- not exhaustive
			- more efficient because we test a new value for every feature on every run
- **explain different hyperparameters of `GridSearchCV`**
	- `param_grid`: set of hyperparameters to be tested
	- `scoring`: metric used to evaluate the performance of the models
	- `cv`: cross-validation splitting strategy
	- `n_jobs`: number of jobs to run in parallel
- **explain the importance of selecting a good range for the values**
	- if range is too narrow, you miss optimal values
	- if range is too wide, it will take too much time and resources
	- range can be selected based on prior knowledge of the model and practical considerations of resources
- **explain optimization bias**
	- occurs when hyperparameter tuning is done on the same dataset used for model evaluation
		- can lead to overfitting
- **identify and reason when to trust and not trust reported accuracies** 